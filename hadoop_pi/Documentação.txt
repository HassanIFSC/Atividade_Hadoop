		Já tendo o Docker Desktop instalado e configurado, se cria uma pasta chamada "hadoop_pi" no diretório que desejar, então 	seguimos os passos para a montagem dos arquivos e o uso via terminal:


	Cria-se dois arquivos: O "docker-compose.yaml" e o "config", nos quais, respectivamente, 
	adicionaremos os nós mestre e operadores, e as configurações do hadoop.


	No arquivo "config", temos o seguinte conteúdo:

	""""

	CORE-SITE.XML_fs.default.name=hdfs://namenode
	CORE-SITE.XML_fs.defaultFS=hdfs://namenode
	HDFS-SITE.XML_dfs.namenode.rpc-address=namenode:8020
	HDFS-SITE.XML_dfs.replication=1
	MAPRED-SITE.XML_mapreduce.framework.name=yarn
	MAPRED-SITE.XML_yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
	MAPRED-SITE.XML_mapreduce.map.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
	MAPRED-SITE.XML_mapreduce.reduce.env=HADOOP_MAPRED_HOME=$HADOOP_HOME
	YARN-SITE.XML_yarn.resourcemanager.hostname=resourcemanager
	YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled=false
	YARN-SITE.XML_yarn.nodemanager.delete.debug-delay-sec=600
	YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled=false
	YARN-SITE.XML_yarn.nodemanager.aux-services=mapreduce_shuffle
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-applications=10000
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-am-resource-percent=0.1
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.queues=default
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.capacity=100
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.user-limit-factor=1
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.maximum-capacity=100
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.state=RUNNING
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_submit_applications=*
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_administer_queue=*
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.node-locality-delay=40
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings=
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings-override.enable=false

	"""",


	Aqui recomenda-se trocar o valor "$HADOOP_HOME" por "/opt/hadoop/", a fim de evitar o empecilho de indicar o local onde estão 	localizados os exemplos de trabalho de forma manual no arquivo "mapred-site.xml" toda vez que for iniciar o cluster


	O novo arquivo ficará assim:
	
	""""

	CORE-SITE.XML_fs.default.name=hdfs://namenode
	CORE-SITE.XML_fs.defaultFS=hdfs://namenode
	HDFS-SITE.XML_dfs.namenode.rpc-address=namenode:8020
	HDFS-SITE.XML_dfs.replication=1
	MAPRED-SITE.XML_mapreduce.framework.name=yarn
	MAPRED-SITE.XML_yarn.app.mapreduce.am.env=HADOOP_MAPRED_HOME=/opt/hadoop/
	MAPRED-SITE.XML_mapreduce.map.env=HADOOP_MAPRED_HOME=/opt/hadoop/
	MAPRED-SITE.XML_mapreduce.reduce.env=HADOOP_MAPRED_HOME=/opt/hadoop/
	YARN-SITE.XML_yarn.resourcemanager.hostname=resourcemanager
	YARN-SITE.XML_yarn.nodemanager.pmem-check-enabled=false
	YARN-SITE.XML_yarn.nodemanager.delete.debug-delay-sec=600
	YARN-SITE.XML_yarn.nodemanager.vmem-check-enabled=false
	YARN-SITE.XML_yarn.nodemanager.aux-services=mapreduce_shuffle
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-applications=10000
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.maximum-am-resource-percent=0.1
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.queues=default
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.capacity=100
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.user-limit-factor=1
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.maximum-capacity=100
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.state=RUNNING
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_submit_applications=*
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.root.default.acl_administer_queue=*
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.node-locality-delay=40
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings=
	CAPACITY-SCHEDULER.XML_yarn.scheduler.capacity.queue-mappings-override.enable=false
	
	""""


	Quanto ao arquivo "docker-compose.yaml", temos as seguintes linhas de código:

	""""

	version: "2"
	services:
   		namenode:
      		image: apache/hadoop:3
      		hostname: namenode
      		command: ["hdfs", "namenode"]
			ports:
			- 9870:9870
			env_file:
			- ./config
			environment:
				ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
   		datanode:
      		image: apache/hadoop:3
      		command: ["hdfs", "datanode"]
      		env_file:
			- ./config
   		datanode01:
      		image: apache/hadoop:3
      		command: ["hdfs", "datanode"]
      		env_file:
			- ./config 
		datanode02:
			image: apache/hadoop:3
      		command: ["hdfs", "datanode"]
      		env_file:
			- ./config
   		resourcemanager:
      		image: apache/hadoop:3
      		hostname: resourcemanager
      		command: ["yarn", "resourcemanager"]
			ports:
			- 8088:8088
			env_file:
			- ./config
			volumes:
			- ./test.sh:/opt/test.sh
		nodemanager:
      		image: apache/hadoop:3
      		command: ["yarn", "nodemanager"]
      		env_file:
			- ./config

	""""


	Com os arquivos com o nó mestre, e os nós operadores, rodamos o comando "docker-compose up -d", 
	aparecendo as seguintes linhas no terminal:

	""""

	[+] Running 7/7
 	✔ Network hadoop_pi_default              Created                                                                  0.0s
 	✔ Container hadoop_pi-nodemanager-1      Started                                                                  1.2s
 	✔ Container hadoop_pi-namenode-1         Started                                                                  1.6s
 	✔ Container hadoop_pi-resourcemanager-1  Started                                                                  1.8s
 	✔ Container hadoop_pi-datanode2-1        Started                                                                  1.2s
 	✔ Container hadoop_pi-datanode1-1        Started                                                                  1.2s
 	✔ Container hadoop_pi-datanode3-1        Started                                                                  1.6s
	
	""""


	Ainda no terminal, executamos:


	""""

	docker exec -it hadoop_pi-namenode-1 /bin/bash

	""""


	Seu terminal mostrará a seguinte linha:


	""""

	bash-4.2$ 
	
	""""


	Então, damos o comando final:


	"yarn jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 16 16"


	Exemplo de saída esperada:

	""""
	Job Finished in 30.274 seconds
	Estimated value of Pi is 3.14062500000000000000

	""""

	Você pode ver mais sobre seu cluster através da aba "Containers" do Docker Desktop.